---
layout: post
title: Deciphering Big Data
subtitle: Each post has a subtitle
categories: Modules
tags: [Github, Modules, Python]
---

### 1. A summary of the learning outcomes of the module
•	Study various concepts of big data, technologies, and data management to enable students to be able to identify and manage challenges associated with security risks and limitations.
	
• Critically analyse data wrangling problems and determine appropriate methodologies and tools in problem solving.

•	Explore different data types and formats. 

•	Explore and evaluate various data storage formats ranging from structured, quasi structured, semi structured, and unstructured formats.

•	Essentially examine different data collection methods and sources. 

•	Review methods to determine the integrity, reliability and readiness of data extracted and presented for pre-processing, cleaning, and usage.

•	Examine data exploration methods and analyse data for presentation in an organisation. 

•	Critically evaluate data readability, readiness, and longevity within the data Pipeline. 

•	Examine cloud services, for example the API (Application Programming Interfaces) and how this enables data interoperability and connectivity.

•	Examine and analyse the ideas and theoretical concepts underlying DBMS (Database Management Systems) Database Design and Modelling.

•	Conduct a deep analysis of the future of use of data and deciphering by examining some fundamental ideas and concepts of machine learning and how these concepts are applied in various methods in handling big data.

## 2.	The artefacts created during the module

### 2.1  Unit 1, 2 and 3: Collaboration discussion
Critically evaluate the rationale behind the Internet of Things (IOT), in the context of, highlighting the opportunities, limitations, risks and challenges associated with such a large-scale process of data collection.

### 2.1.1 Initial Post 
### Banking sector IoT data collection
Kopetz and Steiner  (2022) state that IoT is a  connection of physical things to the internet and this makes it possible to access remote sensor data and to control the physical world from a distance. The logic behind the IoT in a banking sector focused on enhancing operational efficiency, customer experience, data driven decision making and availability of real time data for business processes. This idea is tone down by significant limitations, risks and challenges. 

### Opportunities 
There is real-time data collection from ATMs, branches, mobile apps, and wearables to improve service delivery to customers. The IoT allows for integration of physical and digital banking environments, allowing banks to personalise services and optimise resource allocation. The IoT sensors and biometric devices improve surveillance and authentication, reducing fraud risk.

### Limitations
Despite the big improvement and  promises by the IoT in the banking sector, banks still face several constraints such as infrastructure gaps where in regions like South Africa, inconsistent connectivity, and power supply limit IoT deployment. Based on this, banks may struggle to process and extract actionable insights from vast, unstructured IoT data streams. Poorly cleaned data, missing data, and inaccurate data may lead to incorrect decision making in a banking environment.

### Risks 
Large-scale data collection introduces serious concerns in the IoT platform for example, privacy and consent where continuous monitoring of customer behaviour raises ethical questions about informed consent and data minimisation. The IoT devices are often poorly secured, creating entry points for attackers targeting financial systems. It is imperative that data cleaning to check for extreme values or outliers is effectively done because, if this matter is not dealt with before the analysis, the results will be skewed and unreliable.

### Challenges
The main challenge of IoT is digital inequality in financial exclusion which happens in rural or low income populations where lack of access to IoT platforms or poor connectivity is experienced. Proper and credible techniques must be used to deal with unit nonresponse from data collected thought the IoT platform. The ethical use of AI in leveraging IoT data for automated decision making for example loan approvals will result in high bias and discrimination if this is not properly managed.

### Conclusion
Larger scale IoT applications offer transformation opportunities in efficiency, innovation, and sustainability. To achieve these benefits, one requires a multidimensional strategy that addresses security, interoperability, scalability, regulatory compliance, and human factors. 

### References
Greengard, S., 2021. The internet of things. MIT press.

Hossein Motlagh, N., Mohammadrezaei, M., Hunt, J. and Zakeri, B., 2020. Internet of Things (IoT) and the energy sector. Energies, 13(2), p.494.

Kopetz, H. and Steiner, W., 2022. Internet of things. In Real-time systems: design principles for distributed embedded applications (pp. 325-341). Cham: Springer International Publishing.

Li, F., Li, F., Li, S. and Long, Y., 2020. Deciphering the recreational use of urban parks: Experiments using multi-source big data for all Chinese cities. Science of the Total Environment, 701, p.134896.

Sadhu, P.K., Yanambaka, V.P. and Abdelgawad, A., 2022. Internet of things: Security and solutions survey. Sensors, 22(19), p.7433.

### 2.1.2 Summary Post
The IoT connects physical devices to the internet, enabling real-time data access and remote control. For banks, IoT promises efficiency, customer-centric innovation, and data-driven decision-making. Yet, adoption requires careful navigation of infrastructure realities, regulatory compliance, and ethical considerations. (Fahmideh et al., 2023) state that the IoT platforms are key enablers for smart city initiatives, targeting the improvement of citizens’ quality of life and economic growth. As IoT platforms are dynamic, proactive, and heterogeneous socio-technical artefacts, systematic approaches are required for their development. Based on this statement from Fahmideh et al, there is limited information available from surveys or other sources indicating how IoT platforms are developed and maintained from the perspective of information system development process lifecycle. 

One peer response was received from Preh Muneer Abbasi for the collaboration discussion. The peer review response agreed with all the areas that were discussed in the initial post such as connectivity gaps, poor data quality, security measures. privacy safeguards, fairness controls, and infrastructure investment. Preh Muneer Abbasi suggested that “strategic initiatives are required to close the digital divide by investing in reliable networks and power supply, so that rural or low-income populations are not left behind”. This suggestion is very key in the advancement of the IoT platforms to address infrastructure challenges and digital inequality.

The summary learnings from unit 1 to 3 of the module includes big data, technologies associated with big data, and concepts underlying data management. A deep analysis in understand the four Vs of big data that include volume, velocity, variety, and veracity were also explored. Topics on different data types and formats and how these impact the way data is stored from different organisations were studied. An introduction to the implementation of Python routines and applications using APIs was achieved. Lastly, an in depth study of various data sources, data collection methods, and evaluating challenges associated with data collection process such as item nonresponse, unit nonresponse, missing data, outliers, and duplicate values was offered in the first 3 units of the module.

To sum up, it is apparent that IoT offers transformative opportunities across the world, but organisations must balance innovation with robust governance, cybersecurity frameworks, and compliance alignment to ensure sustainable adoption at all levels.

### References

Bandyopadhyay, D. and Sen, J., 2011. Internet of things: Applications and challenges in technology and standardization. Wireless personal communications, 58(1), pp.49-69.

Fahmideh, M. and Zowghi, D., 2020. An exploration of IoT platform development. Information Systems, 87, p.101409.

Holler, J., Tsiatsis, V., Mulligan, C., Karnouskos, S., Avesand, S. and Boyle, D., 2014. Internet of things. Academic Press.

Kumar, S., Kumar, K.A. and Raman, R., 2021, September. Internet of Things Security: Attacks, Solutions, Strengths and Limitations. In 2021 International 
Conference on Artificial Intelligence and Machine Vision (AIMV) (pp. 1-6). IEEE.

Li, S., Xu, L.D. and Zhao, S., 2015. The internet of things: a survey. Information systems frontiers, 17(2), pp.243-259.

Mukhopadhyay, S.C. and Suryadevara, N.K., 2014. Internet of things: Challenges and opportunities. Internet of things: Challenges and opportunities, pp.1-17.

### 2.1.3 Feedback from the Tutor for the collaboration discussion 
To be completed

### 2.2 Unit 3: Web Scraping
### 2.2.1 Web scraping
### Findings based on Python web scraping code
The Python script was specifically tailor for South African job boards such as Careers24 to align it with local compliance. The code is designed to scrape job postings for the keyword “Data Scientist” and it specifically looked at:

•	Job title as Data Scientist or Senior Data Scientist.

•	Company name for example Capitec Bank, Standard Bank or Deloitte.

•	Location went through different locations in the country, for example Cape Town in Western Cape or Johannesburg in Gauteng
The above attributes are stored in a Python list of dictionaries like:

python

[

  {
    "title": "Data Scientist",
    "company": "Capitec Bank",
    "location": "Cape Town, Western Cape"
  },
  
]

The output is in two formats as outlined below:

1.	JSON file (data_scientist_jobs.json)
   
•	This file format is easy for developers, analysts, or APIs to consume.

#### Example:

json
[

  {
    
    "title": "Data Scientist",
    "company": "Capitec Bank",
    "location": "Cape Town, Western Cape"
    
  }
  
]

2.	XML file (data_scientist_jobs.xml)
   
•	This file format is useful for legacy systems or compliance reporting.

#### Example:
xml

<Jobs>
  
  <Job>
    
    <Title>Data Scientist</Title>
    
    <Company>Capitec Bank</Company>
    
    <Location>Cape Town, Western Cape</Location>
    
  </Job>
  
</Jobs>

### 2.2.2 Important issues to note
1.	The actual findings depend on the live HTML structure of Careers24 or PNet at the time of scraping. If the site changes its CSS classes, for example job card or job title, the script must be updated to align with the changes.
   
2.	Some sites blocked scraping unless a user agent header is included on the code.
   
3.	It is noted that the Python script does not capture salary, posting date, or job description because these variables were not included on the selection list.
   
4.	I need skills in XML and HTML in order to develop an efficient script and be able to correctly interpret the results.

#### The Python script and results (JSON and XML) can be found on the link below.

https://colab.research.google.com/drive/18Y2WAtSWLQJ2y3JFn0yZU_Dbcvtt_oMP?usp=sharing

### 2.3 Unit 4: Data cleaning and transformation 
### 2.3.1 Lecturecast activities and results 
#### Exercise 1
Identify the raw dataset from GitHub as mn.csv and mn_headers.csv and download them into the desktop.
#### 2.3.2 Create mn.csv
Manually creating the mn.csv file that should contain only the data rows, without headers or variables.

#### Steps:

o	Open the raw dataset in a spreadsheet.

o	Delete the first line (the header row).

o	Save the remaining rows as mn.csv.
 <img width="1036" height="593" alt="image" src="https://github.com/user-attachments/assets/cf1192b5-a544-471d-8a16-b4a399cda509" />

#### 2.3.3 Create mn_headers.csv
Manually creating the mn_headers.csv that should contain only the header row.

#### Steps:

o	Copy the first line (the header row) from the raw dataset.

o	Paste it into a new file.

o	Save that file as mn_headers.csv.
 <img width="1020" height="553" alt="image" src="https://github.com/user-attachments/assets/057ac49a-81a2-4a6b-93e9-14bcec0e8f7f" />

The Python example that reads the two files (mn.csv and mn_headers.csv) separately and then combines them into a single DataFrame for analysis is outlined below.
#### Python script
import pandas as pd

Step 1: Load the headers

headers = pd.read_csv("mn_headers.csv", header=None).iloc[0].tolist()

Step 2: Load the data without headers

data = pd.read_csv("mn.csv", header=None)

Step 3: Assign headers to the data

data.columns = headers

Step 4: Inspect the combined DataFrame

print(data.head())

### 2.4 Unit 5: Seminar
### 2.4.1 Title: Case Study on data investigations
### Western African Ebola virus epidemic (2013–2016) 
•	The Western African Ebola virus epidemic (2013–2016) was the most widespread outbreak of Ebola virus disease (EVD) in history.

•	Causing major loss of life and socioeconomic disruption in the region, mainly in Guinea, Liberia, and Sierra Leone.

•	The  first cases were recorded in Guinea in December 2013.

•	Later, the disease spread to neighbouring Liberia and Sierra Leone, with minor outbreaks occurring elsewhere.

•	It caused significant mortality, with the case fatality rate reported which was initially considered, while the rate among hospitalised patients was 57–59%.

•	The final numbers 28,616 people, including 11,310 deaths, for a case-fatality rate of 40%.

### 2.4.2 Data analysis and results
#### Python code

import pandas as pd

Load the dataset
   
df = pd.read_csv("ebola_2014_2016_clean.csv")

Missing values per country per column

missing_values = df.groupby("Country").apply(lambda x: x.isnull().sum())

print("=== Missing Values per Country per Column ===")

print(missing_values)

print("\n")

Cumulative totals (cases and deaths) per country

cumulative_totals = df.groupby("Country")[[

    "Cumulative no. of confirmed, probable and suspected cases",
    
    "Cumulative no. of confirmed, probable and suspected deaths"
    
]].sum()

print("=== Cumulative Totals per Country ===")

print(cumulative_totals)

print("\n")

Mean and Standard Deviation per country
 
stats = df.groupby("Country")[[

    "Cumulative no. of confirmed, probable and suspected cases",
   
    "Cumulative no. of confirmed, probable and suspected deaths"
   
]].

agg (["mean", "std"])

print("=== Mean and Standard Deviation per Country ===")

print(stats)

### 2.4.3 Results

<img width="877" height="594" alt="image" src="https://github.com/user-attachments/assets/61bebee1-32ba-4161-b151-88f2efcfac27" />

Spain, United States, and United Kingdom are outliers in the dataset since the data is based on Western African Ebola virus epidemic.

<img width="747" height="318" alt="image" src="https://github.com/user-attachments/assets/816ae7be-f4c2-4331-9085-a0e651d73d8b" />

Mali, Guinea, Liberia, Nigeria, and Sierra Leone had the highest percentage of deaths based on recorded cases and deaths while Senegal had no deaths from the three reported cases.

<img width="888" height="284" alt="image" src="https://github.com/user-attachments/assets/bf919138-0fd4-4301-a45e-fd8696d011d3" />

### 2.5 Unit 6: Assignment Project Report on a database design
A team of 4 students was created to position themselves as a team of Software Consultants and Developers contracted to design and build a logical database. The team had to choose an application and the client profile for the project. 

Team was expected to prepare and deliver a design report of the intended development work for the client organisation. The design report had to capture the following:

•	Logical design => data items/entities, attributes of the data items chosen, relationships and associations. Identify and explain the data types used and data formats selected.

•	Produce a proposal of the database build, creating an intended database model design. The team was expected to propose a database management system that will be used for the build, considering the client requirements of storage, user access, and the manipulation and retrieval of data within the proposed database.

•	The team was also expected to critically evaluate the data management pipeline process with regards to discussing the capturing of the data used and detailing its source, documenting how you implemented data cleaning techniques and the stages that have been carried out during the cleaning process.

The proposal document was developed and submitted for grading by 01 December 2025 by the Project and Research Lead, Database Designer, Data Manager, and Data Dictionary Analyst.

### 2.6 Unit 7: The Normalisation and database built tasks including the results
Use the DBD_PCOM7E table provided to execute the normalisation task and then use the tables to design a database.
### 2.6.1 Normalisation and results
### 2.6.1.1 First Normal Form (1NF)
#### Rules:

•	Eliminate repeating groups.

•	Ensure atomic values (no multi-valued attributes).

•	Each row must have a unique primary key.

#### Action taken: Separate each course taken by a student into its own row.

<img width="888" height="648" alt="image" src="https://github.com/user-attachments/assets/6762778c-3270-41b1-bfcd-cac8d6e4a2e3" />

### 2.6.1.2  Second Normal Form (2NF)
#### Rules:

•	Must already have passed the 1NF stage.

•	Remove partial dependencies (non-key attributes depending only on part of a composite key). 

#### Action taken: Split the table into separate tables.

Students (student details independent of courses). 

Courses (course details independent of students).

Enrolments (linking students to courses, exam boards, teachers, and scores).

<img width="654" height="369" alt="image" src="https://github.com/user-attachments/assets/587a2c9a-85b3-48d0-bd37-81147a3928ab" />


<img width="454" height="269" alt="image" src="https://github.com/user-attachments/assets/9a65e8e1-389b-4cda-8586-bec6d303a696" />


<img width="789" height="408" alt="image" src="https://github.com/user-attachments/assets/7950aac6-6788-4fff-9b9b-d5b1e154ad2b" />

### 2.6.1.3 Third Normal Form (3NF)
#### Rules:

•	Must already have passed the 2NF stage.

•	Remove transitive dependencies (non-key attributes depending on other non-key attributes).

#### Action taken: Divide the table into five separate tables.

•	Teacher Name depends on Course (not directly on Student).

•	Exam Board depends on Course and so, there is a need to separate Teachers and Exam Boards into their own tables.


<img width="584" height="235" alt="image" src="https://github.com/user-attachments/assets/dcdb03e7-695f-4e4e-9398-0755ef291f9c" />


<img width="584" height="235" alt="image" src="https://github.com/user-attachments/assets/8344a366-5561-4d58-aa67-5e7b76465d13" />


<img width="584" height="235" alt="image" src="https://github.com/user-attachments/assets/0e0ffd18-d3fe-46e9-ab0e-806b710f7f67" />


<img width="584" height="235" alt="image" src="https://github.com/user-attachments/assets/af10f7d6-8324-4be1-a864-4d9e29eb8b1f" />


<img width="769" height="402" alt="image" src="https://github.com/user-attachments/assets/da16b3c2-01ad-475b-89c6-7417fe662f58" />


### 2.6.2 The database design
Microsoft Access database was selected to design the database for this exercise, which is a desktop-based relational database management system (RDBMS) that helps users create, manage, and analyse structured data using tables, queries, forms, and reports.

<img width="888" height="467" alt="image" src="https://github.com/user-attachments/assets/b5fe6d0e-ab5a-4faf-a211-9394501f1645" />


<img width="889" height="798" alt="image" src="https://github.com/user-attachments/assets/2ad48efa-9e51-44ac-97fe-c79893e295bc" />

## 3.	What exactly have I learnt and how?
### 3.1 What have I learnt from this module?
To be completed

### 3.2 The how part of learning the above
To be completed

## 4.	Professional skills gained and enhanced as a result of the module 
To be completed
